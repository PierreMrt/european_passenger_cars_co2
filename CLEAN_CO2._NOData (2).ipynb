{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf65dc-04bc-4938-8a29-2e35a5c7378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_filename = \"C:\\\\Users\\\\Pc\\\\Downloads\\\\FR_CO2_2023.csv\"\n",
    "output_filename = \"C:\\\\Users\\\\Pc\\\\Downloads\\\\CLEAN_FR_CO2_2023.csv\"\n",
    "titre_graph = \"FR_C02_2023\"\n",
    "\n",
    "df = pd.read_csv(input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d003e3-6be2-4ee2-9d07-63445882fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b38b59-2090-46df-9866-8e5022a6ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_del = ['MMS', 'Enedc (g/km)', 'W (mm)', 'At1 (mm)', 'At2 (mm)', 'Ernedc (g/km)', 'De', 'Vf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f36a3-ebf6-457b-8c3e-9bd20f53a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes à supprimer car pas assez remplies (à valider, on peut peut être aussi inclure IT et Erwltp (g/km))\n",
    "col_to_del.append('RLFI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6612b21-3bd3-4578-8ae8-67f117f34a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes à supprimer car doublons d'information\n",
    "col_to_del.extend(('Mp', 'Mh', 'Man', 'Cr', 'm (kg)', 'Fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a16970-d6f4-406e-9010-2eaf5dd4d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes à supprimer car non pertinentes (infos administratives ou relatives au jeu de données en lui-même)\n",
    "col_to_del.extend(('ID', 'Status', 'r', 'year', 'Tan', 'Va', 'Ve', 'Ct', 'Cr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cb111-2b9e-4843-820d-b9e7197a1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des lignes concernant les véhicules électriques ainsi que des colonnes concernant ces véhicules uniquement\n",
    "df = df[df['Ft'] != 'electric']\n",
    "col_to_del.append('Electric range (km)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89216147-c41a-4e0d-a994-0c0e387f7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppresion de colonnes sélectionnées\n",
    "df = df.drop(col_to_del, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f1c80-e796-421e-9f7a-af6f8ebdcddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppresion des doublons\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b0d5c-c6ef-4e06-a084-5e10002bfb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839f060-2229-4a42-95bc-e9cb9256f985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6addf-1405-496f-b43b-cbb2f17aa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd589644-2d9d-41f9-90e2-0db9a0eb7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Étape 1 : Exclusion des carburants rares\n",
    "exclusions = ['lpg', 'e85', 'ng', 'hydrogen']\n",
    "df_filtered = df[~df['Ft'].isin(exclusions)]\n",
    "\n",
    "# Étape 2 : Suppression des outliers PAR groupe de carburant\n",
    "def remove_outliers_iqr_per_group(df, group_col, value_col):\n",
    "    df_clean = pd.DataFrame()\n",
    "    for name, group in df.groupby(group_col):\n",
    "        Q1 = group[value_col].quantile(0.25)\n",
    "        Q3 = group[value_col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        group_clean = group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "        df_clean = pd.concat([df_clean, group_clean], axis=0)\n",
    "    return df_clean.reset_index(drop=True)\n",
    "\n",
    "df_no_outliers = remove_outliers_iqr_per_group(df_filtered, 'Ft', 'Ewltp (g/km)')\n",
    "\n",
    "\n",
    "\n",
    "df_no_outliers.to_csv(f\"{output_filename}\", index=False)\n",
    "\n",
    "# Étape 3 : Affichage final sans outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_no_outliers, x='Ft', y='Ewltp (g/km)', showfliers=False)\n",
    "\n",
    "plt.title(f'Émissions selon le type de carburant (sans outliers visibles) — fichier : {titre_graph}')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807b10d-57e0-4e8e-bb1e-4681f532d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df_no_outliers, x='ec (cm3)', y='Ewltp (g/km)', hue='Ft', aspect=1.5)\n",
    "plt.title(f'Cylindrée vs Émissions par carburant — fichier : {titre_graph}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864965f-d7be-4809-9b7b-7d139b0d1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_no_outliers, x='ech', y='Ewltp (g/km)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f'Émissions selon les normes environnementales — fichier : {titre_graph}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a1d70-3318-4ded-bac8-1c81309b61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la puissance spécifique\n",
    "df_no_outliers['power_to_weight'] = df_no_outliers['ep (KW)'] / df['Mt']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_no_outliers, x='power_to_weight', y='Ewltp (g/km)', hue='Ft')\n",
    "plt.title(f'Puissance spécifique vs Émissions — fichier : {titre_graph}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fcd3f2-78ee-411e-bdf8-9a4cb0ed1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_polluants = df_no_outliers.sort_values(by='Ewltp (g/km)', ascending=False).head(5)\n",
    "top5_sobres = df_no_outliers.sort_values(by='Ewltp (g/km)', ascending=True).head(5)\n",
    "print(titre_graph)\n",
    "print(\"Top 5 véhicules les plus émetteurs :\")\n",
    "print(top5_polluants[['Mk', 'Cn', 'Ewltp (g/km)', 'Ft']])\n",
    "print(\"\\nTop 5 véhicules les plus sobres :\")\n",
    "print(top5_sobres[['Mk', 'Cn', 'Ewltp (g/km)', 'Ft']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ea0f7-3de9-43c9-84f8-2c162d45803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_emissions_by_brand = df_no_outliers.groupby('Mk')['Ewltp (g/km)'].mean().sort_values()\n",
    "mean_emissions_by_brand.head(10).plot(kind='barh', figsize=(10, 6), title=f'Top 10 Marques les plus sobres — fichier : {titre_graph}')\n",
    "plt.xlabel('Ewltp (g/km)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a500ce2-bdd4-41d5-bc7a-56f30bf250ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_no_outliers.columns = df_no_outliers.columns.str.strip()\n",
    "\n",
    "sns.scatterplot(data=df_no_outliers, \n",
    "                x='Fuel consumption', \n",
    "                y='Ewltp (g/km)', \n",
    "                hue='Ft')\n",
    "\n",
    "plt.title(f'Lien entre consommation et émissions — fichier : {titre_graph}')\n",
    "plt.xlabel('Consommation (L/100km)')\n",
    "plt.ylabel('Émissions CO2 (g/km)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6495ca-033c-4624-bc16-8ee6c5bcd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage éventuel des noms de colonnes\n",
    "df_no_outliers.columns = df_no_outliers.columns.str.strip()\n",
    "\n",
    "\n",
    "# Ajout de la puissance spécifique\n",
    "df_no_outliers['power_to_weight'] = df_no_outliers['ep (KW)'] / df_no_outliers['Mt']\n",
    "\n",
    "# Sélection des colonnes à corréler\n",
    "colonnes_corr = ['Ewltp (g/km)', 'Fuel consumption', 'ec (cm3)', 'ep (KW)', 'Mt', 'power_to_weight']\n",
    "df_no_outliers_corr = df_no_outliers[colonnes_corr].copy()\n",
    "\n",
    "# Calcul de la matrice de corrélation\n",
    "correlation_matrix = df_no_outliers_corr.corr()\n",
    "\n",
    "# Affichage de la heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)\n",
    "plt.title(f\"Matrice de corrélation des variables techniques — fichier : {titre_graph}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd3ad5-978a-41b9-b1ae-89e8e3588a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 👉 Suppose que ton DataFrame s'appelle df_no_outliers\n",
    "# Si besoin, recharge ton CSV puis refais ton filtrage d'outliers.\n",
    "df = df_no_outliers.copy()\n",
    "\n",
    "# Sécurise les colonnes numériques (au cas où il reste des chaînes \"N.A.\")\n",
    "num_cols_to_fix = ['Fuel consumption', 'Ewltp (g/km)', 'ec (cm3)', 'ep (kW)', 'Mt']\n",
    "for c in num_cols_to_fix:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Recrée power_to_weight si besoin\n",
    "if 'power_to_weight' not in df.columns and {'ep (kW)','Mt'}.issubset(df.columns):\n",
    "    df['power_to_weight'] = df['ep (kW)'] / df['Mt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771bbc9-d083-456e-99f1-2b25c3f9596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# --- 1. Préparation des données ---\n",
    "data1 = df[['Fuel consumption','Ewltp (g/km)']].dropna()\n",
    "\n",
    "X = data1[['Fuel consumption']]\n",
    "y = data1['Ewltp (g/km)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Entraînement ---\n",
    "lin1 = LinearRegression()\n",
    "lin1.fit(X_train, y_train)\n",
    "\n",
    "# --- 3. Prédictions & métriques ---\n",
    "y_pred = lin1.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)   # ✅ plus de warning\n",
    "\n",
    "print(\"Coef (slope)       :\", lin1.coef_[0])\n",
    "print(\"Intercept          :\", lin1.intercept_)\n",
    "print(\"R² (test)          :\", round(r2, 4))\n",
    "print(\"RMSE (test, g/km)  :\", round(rmse, 3))\n",
    "\n",
    "# --- 4. Visualisation ---\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train, y_train, color=\"blue\", alpha=0.5, label=\"Train\")\n",
    "plt.scatter(X_test, y_test, color=\"green\", alpha=0.5, label=\"Test\")\n",
    "\n",
    "# Génération d'une plage de valeurs pour tracer la droite\n",
    "x_range = pd.DataFrame(\n",
    "    np.linspace(X['Fuel consumption'].min(), X['Fuel consumption'].max(), 100),\n",
    "    columns=['Fuel consumption']\n",
    ")\n",
    "y_pred_line = lin1.predict(x_range)\n",
    "\n",
    "plt.plot(x_range, y_pred_line, color=\"red\", linewidth=2, label=\"Droite de régression\")\n",
    "\n",
    "plt.xlabel(\"Fuel consumption (L/100km)\")\n",
    "plt.ylabel(\"Ewltp (g/km)\")\n",
    "plt.title(\"Régression linéaire univariée : CO₂ ~ Consommation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faac96d-63a7-4de3-b2c0-2448225b36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# --- 1. Préparation des données ---\n",
    "features = ['Fuel consumption', 'ec (cm3)', 'ep (KW)', 'Mt', 'power_to_weight']\n",
    "target = 'Ewltp (g/km)'\n",
    "\n",
    "data2 = df_no_outliers[features + [target]].dropna()\n",
    "\n",
    "X = data2[features]\n",
    "y = data2[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Entraînement ---\n",
    "lin2 = LinearRegression()\n",
    "lin2.fit(X_train, y_train)\n",
    "\n",
    "# --- 3. Prédictions & métriques ---\n",
    "y_pred = lin2.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"=== Résultats Régression Multivariée ===\")\n",
    "print(\"Coefficients :\", dict(zip(features, lin2.coef_)))\n",
    "print(\"Intercept    :\", lin2.intercept_)\n",
    "print(\"R² (test)    :\", round(r2, 4))\n",
    "print(\"RMSE (g/km)  :\", round(rmse, 3))\n",
    "\n",
    "# --- 4. Visualisation (scatter y_test vs y_pred) ---\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color=\"blue\", label=\"Prédictions\")\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color=\"red\", linestyle=\"--\", linewidth=2, label=\"Idéal\")\n",
    "plt.xlabel(\"Valeurs réelles (Ewltp g/km)\")\n",
    "plt.ylabel(\"Valeurs prédites (Ewltp g/km)\")\n",
    "plt.title(\"Régression linéaire multivariée : réel vs prédit\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909535ac-ffc0-437e-a2b8-da52d8026055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# --- 1. Variables numériques + catégorielles ---\n",
    "num_features = ['Fuel consumption', 'ec (cm3)', 'ep (KW)', 'Mt', 'power_to_weight']\n",
    "cat_features = ['Ft', 'ech']   # type de carburant + norme Euro\n",
    "target = 'Ewltp (g/km)'\n",
    "\n",
    "data3 = df_no_outliers[num_features + cat_features + [target]].dropna()\n",
    "\n",
    "# Encodage One-Hot des variables catégorielles\n",
    "data3_encoded = pd.get_dummies(data3, columns=cat_features, drop_first=True)\n",
    "\n",
    "X = data3_encoded.drop(columns=[target])\n",
    "y = data3_encoded[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Entraînement ---\n",
    "lin3 = LinearRegression()\n",
    "lin3.fit(X_train, y_train)\n",
    "\n",
    "# --- 3. Prédictions & métriques ---\n",
    "y_pred = lin3.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"=== Résultats Régression Multivariée (avec catégorielles) ===\")\n",
    "print(\"R² (test)    :\", round(r2, 4))\n",
    "print(\"RMSE (g/km)  :\", round(rmse, 3))\n",
    "\n",
    "# Pour voir les coefficients\n",
    "coefs = pd.DataFrame({\n",
    "    \"Variable\": X.columns,\n",
    "    \"Coefficient\": lin3.coef_\n",
    "}).sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nCoefficients les plus influents :\")\n",
    "print(coefs.head(15))\n",
    "\n",
    "# --- 4. Visualisation ---\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color=\"blue\", label=\"Prédictions\")\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color=\"red\", linestyle=\"--\", linewidth=2, label=\"Idéal\")\n",
    "plt.xlabel(\"Valeurs réelles (Ewltp g/km)\")\n",
    "plt.ylabel(\"Valeurs prédites (Ewltp g/km)\")\n",
    "plt.title(\"Régression multivariée (numériques + catégorielles)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361df1f9-b5d3-42d4-ae3f-8514cecc28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# --- 1) Sélection des colonnes utiles (évite Country, Mk, VFN, dates, etc.) ---\n",
    "target = 'Ewltp (g/km)'\n",
    "num_features = ['Fuel consumption', 'ec (cm3)', 'ep (KW)', 'Mt', 'power_to_weight']\n",
    "cat_features = ['Ft', 'ech']   # on encode seulement ces deux catégories\n",
    "\n",
    "# coerce au cas où il reste des strings numériques\n",
    "dfm = df_no_outliers[num_features + cat_features + [target]].copy()\n",
    "for c in num_features + [target]:\n",
    "    dfm[c] = pd.to_numeric(dfm[c], errors='coerce')\n",
    "\n",
    "# drop lignes incomplètes sur ces colonnes\n",
    "dfm = dfm.dropna(subset=[target] + num_features + cat_features)\n",
    "\n",
    "# --- 2) Encodage One-Hot des catégorielles choisies ---\n",
    "df_encoded = pd.get_dummies(dfm, columns=cat_features, drop_first=True)\n",
    "\n",
    "X = df_encoded.drop(columns=[target])\n",
    "y = df_encoded[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3) Modèle linéaire ---\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "\n",
    "r2_lin = r2_score(y_test, y_pred_lin)\n",
    "rmse_lin = mean_squared_error(y_test, y_pred_lin, squared=False)\n",
    "\n",
    "# --- 4) Random Forest ---\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "\n",
    "print(\"=== Régression Linéaire ===\")\n",
    "print(f\"R² (test): {r2_lin:.4f}\")\n",
    "print(f\"RMSE (g/km): {rmse_lin:.3f}\")\n",
    "\n",
    "print(\"\\n=== Random Forest ===\")\n",
    "print(f\"R² (test): {r2_rf:.4f}\")\n",
    "print(f\"RMSE (g/km): {rmse_rf:.3f}\")\n",
    "\n",
    "# --- 5) Visualisation comparée ---\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred_lin, alpha=0.4, label=f\"Linéaire (R²={r2_lin:.3f})\")\n",
    "plt.scatter(y_test, y_pred_rf,  alpha=0.4, label=f\"RandomForest (R²={r2_rf:.3f})\")\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label=\"Idéal\")\n",
    "plt.xlabel(\"Valeurs réelles (Ewltp g/km)\")\n",
    "plt.ylabel(\"Valeurs prédites (Ewltp g/km)\")\n",
    "plt.title(\"Comparaison : Linéaire vs RandomForest\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6) Importances de variables (Random Forest) ---\n",
    "imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 15 features (RandomForest):\")\n",
    "print(imp.head(15))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "imp.head(15).iloc[::-1].plot(kind='barh')\n",
    "plt.title(\"Importances de variables (RF) – Top 15\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d37fe-a357-4d2f-a169-e6276589d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Sélection des variables ---\n",
    "features_cluster = ['Fuel consumption', 'Mt', 'ep (KW)']\n",
    "df_cluster = df_no_outliers[features_cluster + ['Ewltp (g/km)']].dropna()\n",
    "\n",
    "# --- 2) Normalisation des données (important pour K-Means) ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cluster[features_cluster])\n",
    "\n",
    "# --- 3) Choix du nombre de clusters (méthode du coude) ---\n",
    "inertia = []\n",
    "K_range = range(2, 10)  # on teste entre 2 et 9 clusters\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_scaled)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(K_range, inertia, 'o-')\n",
    "plt.xlabel(\"Nombre de clusters (k)\")\n",
    "plt.ylabel(\"Inertie (within-cluster SSE)\")\n",
    "plt.title(\"Méthode du coude pour choisir k\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4) Appliquer K-Means avec un k choisi (ex: 3) ---\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "df_cluster['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# --- 5) Visualisation 2D (Fuel consumption vs Power) ---\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(data=df_cluster, \n",
    "                x='Fuel consumption', y='ep (KW)', \n",
    "                hue='Cluster', palette='Set1', alpha=0.6)\n",
    "plt.title(\"Clusters de véhicules (conso vs puissance)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 6) Analyse des clusters ---\n",
    "cluster_summary = df_cluster.groupby('Cluster').agg({\n",
    "    'Fuel consumption':'mean',\n",
    "    'Mt':'mean',\n",
    "    'ep (KW)':'mean',\n",
    "    'Ewltp (g/km)':'mean',\n",
    "    'Fuel consumption':'count'\n",
    "}).rename(columns={'Fuel consumption':'Nb_vehicules'})\n",
    "\n",
    "print(\"\\nRésumé par cluster :\")\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fa2e6-b233-4be3-81e6-753461f00028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si besoin: !pip install plotly==5.* scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "# --- 1) Préparer les données & (re)faire KMeans si nécessaire ---\n",
    "features3d = ['Fuel consumption', 'ep (KW)', 'Mt']\n",
    "aux_cols   = ['Ewltp (g/km)', 'Ft', 'ech', 'Mk']  # juste pour les infos au survol si dispo\n",
    "\n",
    "df3d = df_no_outliers[features3d + [c for c in aux_cols if c in df_no_outliers.columns]].dropna().copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df3d[features3d])\n",
    "\n",
    "k = 3  # adapte si tu as choisi un autre k\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "df3d['Cluster'] = kmeans.fit_predict(X_scaled).astype(str)  # str pour une légende propre\n",
    "\n",
    "# --- 2) Nuage 3D interactif ---\n",
    "hover_cols = [c for c in ['Ewltp (g/km)', 'Ft', 'ech', 'Mk'] if c in df3d.columns]\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df3d,\n",
    "    x='Fuel consumption', y='ep (KW)', z='Mt',\n",
    "    color='Cluster',\n",
    "    hover_data=hover_cols,\n",
    "    title=\"Clusters de véhicules (3D) — Consommation vs Puissance vs Masse\",\n",
    "    opacity=0.7\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title=\"Fuel consumption (L/100km)\",\n",
    "    yaxis_title=\"Power (kW)\",\n",
    "    zaxis_title=\"Weight (kg)\"\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320892b-a414-4047-bd94-c841417826d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # nécessaire pour l'affichage 3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- 1) Données & clustering ---\n",
    "features3d = ['Fuel consumption', 'ep (KW)', 'Mt']\n",
    "df3d = df_no_outliers[features3d + ['Ewltp (g/km)']].dropna().copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df3d[features3d])\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df3d['Cluster'] = clusters\n",
    "\n",
    "# (option) échantillonner pour lisibilité si dataset énorme\n",
    "sample = min(20000, len(df3d))\n",
    "dfp = df3d.sample(sample, random_state=42) if len(df3d) > sample else df3d\n",
    "\n",
    "# --- 2) Plot 3D statique ---\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# palette simple\n",
    "palette = ['tab:blue', 'tab:green', 'tab:red', 'tab:orange', 'tab:purple']\n",
    "\n",
    "for c in sorted(dfp['Cluster'].unique()):\n",
    "    sub = dfp[dfp['Cluster'] == c]\n",
    "    ax.scatter(\n",
    "        sub['Fuel consumption'], sub['ep (KW)'], sub['Mt'],\n",
    "        s=8, alpha=0.7, label=f'Cluster {c}', color=palette[c % len(palette)]\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Fuel consumption (L/100km)')\n",
    "ax.set_ylabel('Power (kW)')\n",
    "ax.set_zlabel('Weight (kg)')\n",
    "ax.set_title('Clusters de véhicules (3D) — Consommation vs Puissance vs Masse')\n",
    "\n",
    "# angle de vue (ajuste si besoin)\n",
    "ax.view_init(elev=22, azim=35)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac37a2b-aee9-4157-a1d8-1559b66d6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- Reconstruction d'un DataFrame à partir de X_scaled ---\n",
    "X_clustered = pd.DataFrame(\n",
    "    X_scaled,\n",
    "    columns=['Fuel consumption', 'ep (kW)', 'Mt']\n",
    ")\n",
    "X_clustered['Cluster'] = kmeans.labels_.astype(int)\n",
    "\n",
    "# Prépare les masques/clusters et une palette stable\n",
    "cluster_ids = sorted(X_clustered['Cluster'].unique())\n",
    "colors = {cid: col for cid, col in zip(cluster_ids, ['purple', 'green', 'red', 'orange', 'blue'])}\n",
    "\n",
    "# --- Liste des angles (élévation, azimut) ---\n",
    "angles = [\n",
    "    (20, 45),\n",
    "    (35, 210),\n",
    "    (10, 90),\n",
    "    (40, 300),\n",
    "]\n",
    "\n",
    "# --- Plot ---\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i, (elev, azim) in enumerate(angles, 1):\n",
    "    ax = fig.add_subplot(2, 2, i, projection='3d')\n",
    "\n",
    "    # Scatter par cluster (pour pouvoir mettre une légende catégorielle)\n",
    "    for cid in cluster_ids:\n",
    "        sub = X_clustered[X_clustered['Cluster'] == cid]\n",
    "        ax.scatter(\n",
    "            sub['Fuel consumption'], sub['ep (kW)'], sub['Mt'],\n",
    "            s=20, alpha=0.6, color=colors[cid], label=f\"Cluster {cid}\"\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Fuel consumption (L/100km)\")\n",
    "    ax.set_ylabel(\"Power (kW)\")\n",
    "    ax.set_zlabel(\"Weight (kg)\")\n",
    "    ax.set_title(f\"Vue {i} (elev={elev}, azim={azim})\")\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    # Légende (catégorielle) — retire la colorbar\n",
    "    ax.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018479bf-67c0-49a7-aa8b-dbb1e89930e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # nécessaire pour l'affichage 3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- 1) Données & clustering ---\n",
    "features3d = ['Fuel consumption', 'ep (KW)', 'Mt']\n",
    "df3d = df_no_outliers[features3d + ['Ewltp (g/km)']].dropna().copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df3d[features3d])\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df3d['Cluster'] = clusters\n",
    "\n",
    "# (option) échantillonner pour lisibilité si dataset énorme\n",
    "sample = min(20000, len(df3d))\n",
    "dfp = df3d.sample(sample, random_state=42) if len(df3d) > sample else df3d\n",
    "\n",
    "# --- 2) Plot 3D statique ---\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# palette simple\n",
    "palette = ['tab:blue', 'tab:green', 'tab:red', 'tab:orange', 'tab:purple']\n",
    "\n",
    "for c in sorted(dfp['Cluster'].unique()):\n",
    "    sub = dfp[dfp['Cluster'] == c]\n",
    "    ax.scatter(\n",
    "        sub['Fuel consumption'], sub['ep (KW)'], sub['Mt'],\n",
    "        s=8, alpha=0.7, label=f'Cluster {c}', color=palette[c % len(palette)]\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Fuel consumption (L/100km)')\n",
    "ax.set_ylabel('Power (kW)')\n",
    "ax.set_zlabel('Weight (kg)')\n",
    "ax.set_title('Clusters de véhicules (3D) — Consommation vs Puissance vs Masse')\n",
    "\n",
    "# angle de vue (ajuste si besoin)\n",
    "ax.view_init(elev=10, azim=90)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc9352-65e4-4428-9d07-c0895322faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "# -------- 1) Préparer exactement le même DF pour clustering & plots --------\n",
    "features = ['Fuel consumption', 'ep (KW)', 'Mt']\n",
    "dfc = df_no_outliers[features + ['Ewltp (g/km)']].dropna().copy()\n",
    "\n",
    "# Standardisation pour l'entraînement KMeans (mais pas pour les plots)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(dfc[features])\n",
    "\n",
    "# Clustering (fixe le hasard)\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Ajouter les labels AU DF EN UNITÉS RÉELLES\n",
    "dfc['Cluster'] = labels.astype(int)\n",
    "\n",
    "# Palette stable (même mappage 2D/3D)\n",
    "cluster_ids = sorted(dfc['Cluster'].unique())\n",
    "palette = {cid: col for cid, col in zip(cluster_ids, ['tab:blue','tab:green','tab:red','tab:orange','tab:purple'])}\n",
    "\n",
    "# -------- 2) Plot 2D (mêmes points, mêmes labels, mêmes couleurs) --------\n",
    "plt.figure(figsize=(7,5))\n",
    "for cid in cluster_ids:\n",
    "    sub = dfc[dfc['Cluster']==cid]\n",
    "    plt.scatter(sub['Fuel consumption'], sub['ep (KW)'], s=12, alpha=0.6, color=palette[cid], label=f\"Cluster {cid}\")\n",
    "plt.xlabel(\"Fuel consumption (L/100km)\")\n",
    "plt.ylabel(\"Power (kW)\")\n",
    "plt.title(\"Clusters (2D) – conso vs puissance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- 3) Plot 3D en unités réelles, mêmes couleurs & labels --------\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for cid in cluster_ids:\n",
    "    sub = dfc[dfc['Cluster']==cid]\n",
    "    ax.scatter(sub['Fuel consumption'], sub['ep (KW)'], sub['Mt'],\n",
    "               s=10, alpha=0.6, color=palette[cid], label=f\"Cluster {cid}\")\n",
    "ax.set_xlabel(\"Fuel consumption (L/100km)\")\n",
    "ax.set_ylabel(\"Power (kW)\")\n",
    "ax.set_zlabel(\"Weight (kg)\")\n",
    "ax.set_title(\"Clusters (3D) – conso vs puissance vs masse\")\n",
    "ax.view_init(elev=30, azim=210)  # ajuste si besoin\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16519b1-4674-45a5-bed4-d76b3d47df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- 1) Préparation du DF sur lequel on mettra les labels ---\n",
    "# features utilisées pour le clustering\n",
    "features = ['Fuel consumption', 'ep (KW)', 'Mt']\n",
    "\n",
    "# colonnes meta pour analyses (marque, carburant, norme) si elles existent\n",
    "meta_cols = [c for c in ['Mk', 'Ft', 'ech', 'Ewltp (g/km)'] if c in df_no_outliers.columns]\n",
    "\n",
    "# on part d'un DF propre, en unités réelles\n",
    "dfc = df_no_outliers[features + meta_cols].copy()\n",
    "\n",
    "# conversion sûre des numériques\n",
    "for c in features + ['Ewltp (g/km)']:\n",
    "    if c in dfc.columns:\n",
    "        dfc[c] = pd.to_numeric(dfc[c], errors='coerce')\n",
    "\n",
    "# drop NA sur les features (très important : un seul dropna, réutilisé partout)\n",
    "dfc = dfc.dropna(subset=features).reset_index(drop=True)\n",
    "\n",
    "# --- 2) Clustering sur données standardisées, mais labels ajoutés au DF réel ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(dfc[features])\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "dfc['Cluster'] = labels.astype(int)   # <-- la colonne existe maintenant dans dfc\n",
    "\n",
    "# --- 3) Résumé quanti + Top marques par cluster ---\n",
    "def top_n(series, n=3):\n",
    "    return series.value_counts(dropna=True).head(n).index.tolist()\n",
    "\n",
    "groupers = {\n",
    "    'Fuel consumption': 'mean',\n",
    "    'ep (KW)': 'mean',\n",
    "    'Mt': 'mean'\n",
    "}\n",
    "if 'Ewltp (g/km)' in dfc.columns:\n",
    "    groupers['Ewltp (g/km)'] = 'mean'\n",
    "\n",
    "summary = dfc.groupby('Cluster').agg(groupers)\n",
    "summary.insert(0, 'Nb véhicules', dfc.groupby('Cluster').size())\n",
    "\n",
    "if 'Mk' in dfc.columns:\n",
    "    summary['Top 3 marques'] = (\n",
    "        dfc.groupby('Cluster')['Mk']\n",
    "           .apply(lambda s: top_n(s, 3))\n",
    "    )\n",
    "\n",
    "print(\"=== Résumé par cluster ===\")\n",
    "print(summary)\n",
    "\n",
    "# --- 4) PCA 2D pour visualiser les clusters ---\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "palette = {cid: col for cid, col in zip(sorted(dfc['Cluster'].unique()),\n",
    "                                        ['tab:blue','tab:green','tab:red','tab:orange','tab:purple'])}\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for cid, col in palette.items():\n",
    "    mask = (dfc['Cluster'] == cid)\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], s=10, alpha=0.6, color=col, label=f'Cluster {cid}')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA (2D) des clusters')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7465d-47a2-4e98-974a-f03228aca0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ==== données & features ====\n",
    "dfm = df_no_outliers.copy()\n",
    "num = ['Fuel consumption', 'ec (cm3)', 'ep (KW)', 'Mt', 'power_to_weight']\n",
    "cat = [c for c in ['Ft', 'ech'] if c in dfm.columns]\n",
    "target = 'Ewltp (g/km)'\n",
    "\n",
    "# conversions sûres\n",
    "for c in num+[target]:\n",
    "    if c in dfm.columns:\n",
    "        dfm[c] = pd.to_numeric(dfm[c], errors='coerce')\n",
    "\n",
    "dfm = dfm.dropna(subset=num+[target]+cat)\n",
    "\n",
    "# ==== pipeline linéaire interprétable ====\n",
    "num_tr = Pipeline([(\"scaler\", StandardScaler())])\n",
    "cat_tr = Pipeline([(\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "pre = ColumnTransformer([(\"num\", num_tr, num), (\"cat\", cat_tr, cat)])\n",
    "\n",
    "model = Pipeline([(\"pre\", pre), (\"lin\", LinearRegression())])\n",
    "X = dfm[num+cat]\n",
    "y = dfm[target]\n",
    "model.fit(X, y)\n",
    "\n",
    "# ==== CO2 attendu & sobriété ====\n",
    "dfm[\"CO2_prédit\"] = model.predict(X)\n",
    "dfm[\"sobriete\"] = dfm[target] / dfm[\"CO2_prédit\"]\n",
    "\n",
    "# Top / bottom par modèle (marque + modèle commercial si dispo)\n",
    "group_key = [c for c in ['Mk','Cn'] if c in dfm.columns] or ['VFN']  # fallback\n",
    "rank = (dfm.groupby(group_key)[['sobriete', target, 'CO2_prédit']]\n",
    "          .mean().assign(N=dfm.groupby(group_key).size())\n",
    "          .query(\"N >= 50\")     # filtre: au moins 50 obs par modèle\n",
    "          .sort_values('sobriete'))\n",
    "\n",
    "top_sobres   = rank.head(10)\n",
    "top_energiv  = rank.tail(10)\n",
    "\n",
    "print(\"\\nTop 10 modèles les PLUS sobres (sobriété < 1):\")\n",
    "print(top_sobres)\n",
    "\n",
    "print(\"\\nTop 10 modèles les MOINS sobres (sobriété > 1):\")\n",
    "print(top_energiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad50372-4216-4b67-8000-f2adc9c7351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ===================== 1) Préparer les données =====================\n",
    "dfc = df_no_outliers.copy()\n",
    "\n",
    "num = ['Fuel consumption', 'ec (cm3)', 'ep (KW)', 'Mt', 'power_to_weight']\n",
    "cat = [c for c in ['Ft', 'ech'] if c in dfc.columns]\n",
    "target_cont = 'Ewltp (g/km)'\n",
    "\n",
    "# conversions sûres\n",
    "for c in num + [target_cont]:\n",
    "    if c in dfc.columns:\n",
    "        dfc[c] = pd.to_numeric(dfc[c], errors='coerce')\n",
    "\n",
    "dfc = dfc.dropna(subset=num + cat + [target_cont]).copy()\n",
    "\n",
    "# ===================== 2) Créer les classes CO2 =====================\n",
    "# Seuils \"réglementaires\" simples\n",
    "bins = [0, 95, 120, 150, dfc[target_cont].max()]\n",
    "labels = ['A', 'B', 'C', 'D']\n",
    "dfc['Classe_CO2'] = pd.cut(dfc[target_cont], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Si besoin: vérifier le déséquilibre\n",
    "print(\"Répartition des classes :\\n\", dfc['Classe_CO2'].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "# ===================== 3) Split & pipeline =====================\n",
    "X = dfc[num + cat]\n",
    "y = dfc['Classe_CO2']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "num_tr = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_tr = Pipeline([\n",
    "    (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_tr, num),\n",
    "    (\"cat\", cat_tr, cat),\n",
    "])\n",
    "\n",
    "# ===================== 4) Modèles =====================\n",
    "# 4a) Logistic Regression (baseline)\n",
    "log_clf = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\", multi_class=\"auto\"))\n",
    "])\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_pred_log = log_clf.predict(X_test)\n",
    "\n",
    "# 4b) RandomForestClassifier\n",
    "rf_clf = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# ===================== 5) Évaluation =====================\n",
    "def eval_and_plot(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1w = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"F1 (pondéré) : {f1w:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    disp.ax_.set_title(f\"Matrice de confusion — {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "eval_and_plot(y_test, y_pred_log, \"Logistic Regression\")\n",
    "eval_and_plot(y_test, y_pred_rf, \"RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3efd1-988b-4fdf-a249-156997e658c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ========= 0) Paramètres =========\n",
    "TARGET = 'Fuel consumption'\n",
    "num_cols = ['Mt', 'ec (cm3)', 'ep (KW)', 'power_to_weight']   # 'ep (KW)' (KW en majuscule dans ton DF)\n",
    "cat_cols = [c for c in ['Ft', 'ech'] if c in df_no_outliers.columns]\n",
    "\n",
    "# ========= 1) Copie & nettoyage minimal =========\n",
    "dfc = df_no_outliers.copy()\n",
    "# Crée power_to_weight si manquant\n",
    "if 'power_to_weight' not in dfc.columns and {'ep (KW)','Mt'}.issubset(dfc.columns):\n",
    "    dfc['power_to_weight'] = pd.to_numeric(dfc['ep (KW)'], errors='coerce') / pd.to_numeric(dfc['Mt'], errors='coerce')\n",
    "\n",
    "# conversions sûres des numériques\n",
    "for c in [TARGET] + [col for col in num_cols if col in dfc.columns]:\n",
    "    dfc[c] = pd.to_numeric(dfc[c], errors='coerce')\n",
    "\n",
    "# garde seulement les colonnes existantes\n",
    "num_used = [c for c in num_cols if c in dfc.columns]\n",
    "all_needed = [TARGET] + num_used + cat_cols\n",
    "dfc = dfc[all_needed].dropna().copy()\n",
    "\n",
    "print(\"Num features utilisées :\", num_used)\n",
    "print(\"Cat features utilisées :\", cat_cols)\n",
    "print(\"Taille après nettoyage :\", dfc.shape)\n",
    "\n",
    "# ========= 2) Split =========\n",
    "X = dfc[num_used + cat_cols]\n",
    "y = dfc[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ========= 3) Préprocessing commun =========\n",
    "num_tr = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_tr = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_tr, num_used),\n",
    "        (\"cat\", cat_tr, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ========= 4) Modèles =========\n",
    "lin_pipe = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ========= 5) Entraînement =========\n",
    "lin_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ========= 6) Évaluation =========\n",
    "def evaluate(name, pipe):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"R²    : {r2:.4f}\")\n",
    "    print(f\"RMSE  : {rmse:.3f} L/100km\")\n",
    "    print(f\"MAE   : {mae:.3f} L/100km\")\n",
    "    # Scatter réel vs prédit\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(y_test, y_pred, s=8, alpha=0.5)\n",
    "    lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "    plt.plot(lims, lims, 'r--', lw=2, label='Idéal')\n",
    "    plt.xlabel(\"Conso réelle (L/100km)\")\n",
    "    plt.ylabel(\"Conso prédite (L/100km)\")\n",
    "    plt.title(f\"{name} — Réel vs Prédit\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return y_pred, r2, rmse, mae\n",
    "\n",
    "y_pred_lin, r2_lin, rmse_lin, mae_lin = evaluate(\"Régression linéaire\", lin_pipe)\n",
    "y_pred_rf,  r2_rf,  rmse_rf,  mae_rf  = evaluate(\"RandomForestRegressor\", rf_pipe)\n",
    "\n",
    "# ========= 7) Importances (RF) =========\n",
    "# Récupération des noms de features après encodage pour afficher les importances\n",
    "try:\n",
    "    feat_names = rf_pipe.named_steps['pre'].get_feature_names_out()\n",
    "except Exception:\n",
    "    # compat skearn anciens\n",
    "    num_names = num_used\n",
    "    oh = rf_pipe.named_steps['pre'].named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = oh.get_feature_names_out(cat_cols) if cat_cols else []\n",
    "    feat_names = np.r_[num_names, cat_names]\n",
    "\n",
    "importances = rf_pipe.named_steps['model'].feature_importances_\n",
    "imp = pd.Series(importances, index=feat_names).sort_values(ascending=False)\n",
    "print(\"\\nTop 15 features importantes (RF) :\")\n",
    "print(imp.head(15))\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "imp.head(15).iloc[::-1].plot(kind='barh')\n",
    "plt.title(\"Importances des variables (RandomForest) — Top 15\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a1d37-8ce2-4e5d-bf6d-40eaadd55c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# -------- 1) Données & colonnes --------\n",
    "TARGET = 'Fuel consumption'\n",
    "num_cols = ['Mt', 'ec (cm3)', 'ep (KW)', 'power_to_weight']\n",
    "cat_cols = [c for c in ['Ft', 'ech'] if c in df_no_outliers.columns]\n",
    "\n",
    "dfc = df_no_outliers.copy()\n",
    "\n",
    "# créer power_to_weight si absent\n",
    "if 'power_to_weight' not in dfc.columns and {'ep (KW)','Mt'}.issubset(dfc.columns):\n",
    "    dfc['power_to_weight'] = pd.to_numeric(dfc['ep (KW)'], errors='coerce') / pd.to_numeric(dfc['Mt'], errors='coerce')\n",
    "\n",
    "# conversions numériques robustes\n",
    "for c in [TARGET] + num_cols:\n",
    "    if c in dfc.columns:\n",
    "        dfc[c] = pd.to_numeric(dfc[c], errors='coerce')\n",
    "\n",
    "# garder uniquement ce qui est utile & drop NA une seule fois\n",
    "use_num = [c for c in num_cols if c in dfc.columns]\n",
    "dfc = dfc[[TARGET] + use_num + cat_cols].dropna().copy()\n",
    "\n",
    "X = dfc[use_num + cat_cols]\n",
    "y = dfc[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------- 2) Prétraitement commun --------\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), use_num),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------- 3) Modèles à comparer (dans des Pipelines) --------\n",
    "models = {\n",
    "    \"Linéaire\": Pipeline([(\"pre\", pre), (\"model\", LinearRegression())]),\n",
    "    \"RandomForest\": Pipeline([(\"pre\", pre), (\"model\", RandomForestRegressor(\n",
    "        n_estimators=300, random_state=42, n_jobs=-1))]),\n",
    "    \"GradientBoosting\": Pipeline([(\"pre\", pre), (\"model\", GradientBoostingRegressor(\n",
    "        random_state=42))]),\n",
    "}\n",
    "\n",
    "results = []\n",
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    results.append([name, r2, rmse, mae])\n",
    "\n",
    "    plt.scatter(y_test, y_pred, s=8, alpha=0.5, label=f\"{name} (R²={r2:.3f})\")\n",
    "\n",
    "# diagonale idéale\n",
    "lims = [min(y_test.min(), min([r[2] for r in []], default=y_test.min())), max(y_test.max(), y_test.max())]\n",
    "xline = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "plt.plot(xline, xline, 'r--', label=\"Idéal\")\n",
    "plt.xlabel(\"Conso réelle (L/100km)\")\n",
    "plt.ylabel(\"Conso prédite (L/100km)\")\n",
    "plt.title(\"Comparaison des modèles (prétraitement identique)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- 4) Tableau récap --------\n",
    "df_results = pd.DataFrame(results, columns=[\"Modèle\", \"R²\", \"RMSE (L/100km)\", \"MAE (L/100km)\"])\\\n",
    "               .sort_values(\"R²\", ascending=False)\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334717e8-6236-4b33-b075-7410582f1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose qu'on a déjà entraîné le pipeline RandomForest:\n",
    "# rf_pipe = Pipeline([(\"pre\", pre), (\"model\", RandomForestRegressor(...))])\n",
    "# rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 1) Séparer le préprocesseur et le modèle\n",
    "preproc = rf_pipe.named_steps[\"pre\"]\n",
    "rf      = rf_pipe.named_steps[\"model\"]\n",
    "\n",
    "# 2) Transformer X_test -> matrice numérique\n",
    "X_test_trans = preproc.transform(X_test)\n",
    "\n",
    "# 3) Récupérer les noms de features après encodage (pour des plots lisibles)\n",
    "try:\n",
    "    feature_names = preproc.get_feature_names_out()\n",
    "except Exception:\n",
    "    # compat anciens sklearn\n",
    "    num_names = preproc.named_transformers_[\"num\"].get_feature_names_out()\n",
    "    oh        = preproc.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_names = oh.get_feature_names_out(preproc.transformers_[1][2]) if oh is not None else []\n",
    "    feature_names = np.r_[num_names, cat_names]\n",
    "\n",
    "X_test_trans_df = pd.DataFrame(X_test_trans.toarray() if hasattr(X_test_trans, \"toarray\") else X_test_trans,\n",
    "                               columns=feature_names)\n",
    "\n",
    "# 4) Explainer SHAP pour modèles d’arbres\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "\n",
    "# Astuce : échantillonner pour les plots si beaucoup d’observations\n",
    "X_sample = X_test_trans_df.sample(min(2000, len(X_test_trans_df)), random_state=42)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# 5) Importance globale (bar plot)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP – Importance globale des variables (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Beeswarm (distribution des impacts, couleur = valeur de la feature)\n",
    "shap.summary_plot(shap_values, X_sample, show=False)\n",
    "plt.title(\"SHAP – Distribution des contributions par variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) Explication locale d’une observation\n",
    "i = 0  # change l'index si tu veux une autre voiture\n",
    "sv_i = explainer.shap_values(X_sample.iloc[[i]])\n",
    "shap.force_plot(explainer.expected_value, sv_i, X_sample.iloc[[i]], matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c68b6b-decd-4e14-b41a-583e670f6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# ========= 1) Dataframe: specs techniques uniquement =========\n",
    "df_tech = df_no_outliers.copy()  # <- ton DF nettoyé\n",
    "\n",
    "# Créer power_to_weight si absent\n",
    "if 'power_to_weight' not in df_tech.columns and {'ep (KW)', 'Mt'}.issubset(df_tech.columns):\n",
    "    df_tech['power_to_weight'] = (\n",
    "        pd.to_numeric(df_tech['ep (KW)'], errors='coerce') /\n",
    "        pd.to_numeric(df_tech['Mt'], errors='coerce')\n",
    "    )\n",
    "\n",
    "num_cols = [c for c in ['Mt', 'ec (cm3)', 'ep (KW)', 'power_to_weight'] if c in df_tech.columns]\n",
    "target   = 'Fuel consumption'\n",
    "\n",
    "# Conversions numériques robustes\n",
    "for c in num_cols + [target]:\n",
    "    if c in df_tech.columns:\n",
    "        df_tech[c] = pd.to_numeric(df_tech[c], errors='coerce')\n",
    "\n",
    "# ⚠️ On supprime uniquement les lignes où la CIBLE est NaN (on imputera X ensuite)\n",
    "df_tech = df_tech.dropna(subset=[target]).reset_index(drop=True)\n",
    "\n",
    "X = df_tech[num_cols]\n",
    "y = df_tech[target]\n",
    "\n",
    "print(\"Colonnes utilisées :\", num_cols)\n",
    "print(\"NaN restants dans X (seront imputés):\\n\", X.isna().sum(), \"\\n\")\n",
    "\n",
    "# ========= 2) Split =========\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ========= 3) Prétraitements =========\n",
    "# Linéaire: Imputation (médiane) + Standardisation\n",
    "pre_lin = ColumnTransformer(\n",
    "    [(\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]), num_cols)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# RandomForest: Imputation (médiane) seule\n",
    "pre_rf = ColumnTransformer(\n",
    "    [(\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_cols)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# ========= 4) Modèles =========\n",
    "lin_pipe = Pipeline([(\"pre\", pre_lin), (\"model\", LinearRegression())])\n",
    "\n",
    "rf_pipe = Pipeline([(\"pre\", pre_rf), (\"model\", RandomForestRegressor(\n",
    "    n_estimators=300, random_state=42, n_jobs=-1\n",
    "))])\n",
    "\n",
    "# ========= 5) Entraînement + Évaluation =========\n",
    "def eval_model(name, pipe):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"{name} -> R²={r2:.3f} | RMSE={rmse:.3f} L/100km | MAE={mae:.3f} L/100km\")\n",
    "    return y_pred, r2, rmse, mae\n",
    "\n",
    "y_pred_lin, r2_lin, rmse_lin, mae_lin = eval_model(\"Régression linéaire (tech only)\", lin_pipe)\n",
    "y_pred_rf,  r2_rf,  rmse_rf,  mae_rf  = eval_model(\"RandomForest (tech only)\", rf_pipe)\n",
    "\n",
    "# ========= 6) Scatter comparatif =========\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(y_test, y_pred_lin, s=8, alpha=0.5, label=f\"Linéaire (R²={r2_lin:.3f})\")\n",
    "plt.scatter(y_test, y_pred_rf,  s=8, alpha=0.5, label=f\"RandomForest (R²={r2_rf:.3f})\")\n",
    "lims = [min(y_test.min(), y_pred_lin.min(), y_pred_rf.min()),\n",
    "        max(y_test.max(), y_pred_lin.max(), y_pred_rf.max())]\n",
    "plt.plot(lims, lims, 'r--', label=\"Idéal\")\n",
    "plt.xlabel(\"Conso réelle (L/100km)\")\n",
    "plt.ylabel(\"Conso prédite (L/100km)\")\n",
    "plt.title(\"Conso prédite — Specs techniques uniquement (imputation médiane)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ce74e-e16d-4d91-8115-5c7e6538add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# =============== 1) Données : specs techniques uniquement ===============\n",
    "dfx = df_no_outliers.copy()\n",
    "\n",
    "# power_to_weight si absent\n",
    "if 'power_to_weight' not in dfx.columns and {'ep (KW)', 'Mt'}.issubset(dfx.columns):\n",
    "    dfx['power_to_weight'] = pd.to_numeric(dfx['ep (KW)'], errors='coerce') / pd.to_numeric(dfx['Mt'], errors='coerce')\n",
    "\n",
    "num_cols = [c for c in ['Mt', 'ec (cm3)', 'ep (KW)', 'power_to_weight'] if c in dfx.columns]\n",
    "target   = 'Fuel consumption'\n",
    "\n",
    "# conversions numériques\n",
    "for c in num_cols + [target]:\n",
    "    dfx[c] = pd.to_numeric(dfx[c], errors='coerce')\n",
    "\n",
    "# on ne garde que les lignes avec cible connue (X sera imputé)\n",
    "dfx = dfx.dropna(subset=[target]).reset_index(drop=True)\n",
    "\n",
    "X = dfx[num_cols]\n",
    "y = dfx[target]\n",
    "\n",
    "print(\"Features utilisées :\", num_cols, \"| Taille :\", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prétraitement commun pour arbres : imputation (médiane)\n",
    "preproc = ColumnTransformer(\n",
    "    [(\"num\", SimpleImputer(strategy=\"median\"), num_cols)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# =============== 2) Modèles ===============\n",
    "models = {\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"model\", RandomForestRegressor(\n",
    "            n_estimators=500, max_depth=None,\n",
    "            random_state=42, n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \"GradientBoosting\": Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"model\", GradientBoostingRegressor(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            max_depth=3, subsample=0.9, random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# XGBoost si installé\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    models[\"XGBoost\"] = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"model\", XGBRegressor(\n",
    "            n_estimators=800, learning_rate=0.05, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            objective=\"reg:squarederror\", random_state=42, n_jobs=-1,\n",
    "            tree_method=\"hist\"  # rapide\n",
    "        ))\n",
    "    ])\n",
    "    has_xgb = True\n",
    "except Exception as e:\n",
    "    print(\"⚠️ XGBoost non disponible. Pour l’installer :  pip install xgboost\")\n",
    "    has_xgb = False\n",
    "\n",
    "# =============== 3) Entraînement, évaluation, plots ===============\n",
    "results = []\n",
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    results.append([name, r2, rmse, mae])\n",
    "\n",
    "    plt.scatter(y_test, y_pred, s=8, alpha=0.5, label=f\"{name} (R²={r2:.3f})\")\n",
    "\n",
    "# diagonale idéale\n",
    "lims = [min(y_test.min(), min([r[2] for r in results], default=y_test.min())),\n",
    "        max(y_test.max(), y_test.max())]\n",
    "xline = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "plt.plot(xline, xline, 'r--', label=\"Idéal\")\n",
    "\n",
    "plt.xlabel(\"Conso réelle (L/100km)\")\n",
    "plt.ylabel(\"Conso prédite (L/100km)\")\n",
    "plt.title(\"Comparaison RF vs GradientBoosting vs XGBoost (spécs techniques)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau récapitulatif\n",
    "df_res = pd.DataFrame(results, columns=[\"Modèle\",\"R²\",\"RMSE (L/100km)\",\"MAE (L/100km)\"]).sort_values(\"R²\", ascending=False)\n",
    "print(df_res.to_string(index=False))\n",
    "\n",
    "# =============== 4) Importances des variables du meilleur modèle ===============\n",
    "best_name = df_res.iloc[0][\"Modèle\"]\n",
    "best_pipe = models[best_name]\n",
    "best_model = best_pipe.named_steps[\"model\"]\n",
    "\n",
    "# récup features après imputation\n",
    "Xt = best_pipe.named_steps[\"pre\"].transform(X_test)\n",
    "feat_names = num_cols  # ici pas d'encodage cat, donc simple\n",
    "\n",
    "# importance si le modèle la supporte\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = pd.Series(best_model.feature_importances_, index=feat_names).sort_values(ascending=True)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    importances.plot(kind=\"barh\")\n",
    "    plt.title(f\"Importances — {best_name}\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"(Pas d'importances natives pour {best_name})\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a3ec405-cfa4-4b71-83df-3bacd90cc41a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
